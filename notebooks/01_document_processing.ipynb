{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Workflow\n",
    "\n",
    "This notebook outlines the document processing workflow using AWS Textract to extract text from documents stored in S3. The extracted text will be parsed and prepared for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access successful!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize AWS clients\n",
    "# sts = boto3.client('sts')\n",
    "# print(sts.get_caller_identity())  # Verify which identity you're using\n",
    "s3_client = boto3.client('s3')\n",
    "textract_client = boto3.client('textract')\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket='genaiprojectawsbucket')\n",
    "    print(\"Access successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Document to S3\n",
    "\n",
    "Ensure that the document you want to process is uploaded to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document D:/Universty Files/GenAI_Project/document-qa-aws-project/documents/doc1.pdf uploaded to S3 bucket genaiprojectawsbucket with key doc1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define S3 bucket and document name\n",
    "bucket_name = 'genaiprojectawsbucket'\n",
    "# Local file path\n",
    "local_file = 'D:/Universty Files/GenAI_Project/document-qa-aws-project/documents/doc1.pdf'\n",
    "# S3 key (just use the filename)\n",
    "s3_key = 'doc1.pdf'\n",
    "\n",
    "# Upload document to S3\n",
    "s3_client.upload_file(local_file, bucket_name, s3_key)\n",
    "print(f'Document {local_file} uploaded to S3 bucket {bucket_name} with key {s3_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Text from Document using Textract\n",
    "\n",
    "Use AWS Textract to extract text from the uploaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.37.9\n",
      "botocore version: 1.37.9\n",
      "AWS Identity: arn:aws:iam::438465158896:user/GenAIproject\n",
      "Using region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Print boto3 version\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"botocore version: {botocore.__version__}\")\n",
    "\n",
    "# Initialize clients with explicit region\n",
    "region_name = \"us-east-1\"  # Replace with your region\n",
    "textract_client = boto3.client('textract', region_name=region_name)\n",
    "s3_client = boto3.client('s3', region_name=region_name)\n",
    "\n",
    "# Check AWS identity\n",
    "sts = boto3.client('sts', region_name=region_name)\n",
    "try:\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"AWS Identity: {identity['Arn']}\")\n",
    "    print(f\"Using region: {region_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Identity error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textract job started successfully with JobId: 2023bd0b2e5c5cbdff5753ecba1633caae2bbe3c435209f5c5f3f39e6a77fb75\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = textract_client.start_document_text_detection(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': bucket_name,\n",
    "                'Name': s3_key\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    job_id = response['JobId']\n",
    "    print(f\"Textract job started successfully with JobId: {job_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting Textract job: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Poll for Textract Job Completion\n",
    "\n",
    "We need to wait for the Textract job to complete before we can retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "def check_textract_job(job_id):\n",
    "    response = textract_client.get_document_text_detection(JobId=job_id)\n",
    "    status = response['JobStatus']\n",
    "    return status, response\n",
    "\n",
    "# Poll for job completion\n",
    "while True:\n",
    "    status, response = check_textract_job(job_id)\n",
    "    print(f'Job status: {status}')\n",
    "    if status in ['SUCCEEDED', 'FAILED']:\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse the Extracted Text\n",
    "\n",
    "Once the job is complete, we can parse the extracted text from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted text to S3: doc1_text.txt\n"
     ]
    }
   ],
   "source": [
    "# In notebook 01, add this code to save extracted text:\n",
    "if status == 'SUCCEEDED':\n",
    "    extracted_text = ''\n",
    "    for item in response['Blocks']:\n",
    "        if item['BlockType'] == 'LINE':\n",
    "            extracted_text += item['Text'] + '\\n'\n",
    "\n",
    "    # Save to S3\n",
    "    text_key = 'doc1_text.txt'\n",
    "    s3_client.put_object(\n",
    "        Body=extracted_text,\n",
    "        Bucket=bucket_name,\n",
    "        Key=text_key\n",
    "    )\n",
    "    print(f\"Saved extracted text to S3: {text_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully uploaded a document to S3, extracted text using AWS Textract, and parsed the results. The next steps would involve generating embeddings and integrating this workflow into the overall Q&A system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
